week of 6/25:
  x finish range module
  x load fn docs from separate text files
  make bst a member of listlike
  test cut with busco:
    re-run most conservative
    if time, extend to a range of quality scores
    if time, make a tutorial and assign as homework
  orthogroups:
    design all-vs-all type/module
    write all-vs-all module + integrate with orthogroup algorithms
    write orthogroup result handling while running the ortholog finder cuts
  make it look nice:
    finish busco plots + maybe generic "histogram before/after" plots
    add (or just document?) set membership tables
    get venn diagrams working
  update tests + bump version
  write + record demo for small group (just video, not on demo site yet):
    talk about how to trade off your time vs the computer's time

next week:
  result handling: write standalone file, or list mixing types
  start writing docs for most functions
  test + merge url loading
  implement cheat module (rename Script)
  try leapfrog
  add range tests

list dedup bug:
  is it in extract_queries?
  more generally, i probably need to write a "dedup result" function to run after all scripts
    ideally also a separate script so it can run on a different node?

first stage of the rewrite is to use a record for the wrappedCmd* fns
  should only need one of them since all scripts now?
  should have readable fields instead of positional args

second stage is to clean it up and separate .args/.sh files
  should send stdout + stderr to separate files automatically, but not via haskell?
    i guess this is where the standard wrapper script would step in
  should handle locking + retry + similar hacks outside haskell too?
  should have separate .args/sh files amenable to later mapping (third stage)
  should work in multiple situations: raw, GNU parallel, singularity + SLURM, ideally docker (later)

deduplicate interpreters from different modules? one myPython, myR etc.

flag to tell shortcut how many machines it's running on, and you get that many parallel commands at once?

failing tests:
  crb_blast_each2

include tests:
  include one (two results! not right)
  include two (three results!)
  include nested

deduplicate interpreters from different modules? one myPython, myR etc.

flag to tell shortcut how many machines it's running on, and you get that many parallel commands at once?

rewrite system calls for better reliability (and mapping):
  x attempt to simplify module layout at the same time
  think about adding a wrappedCmd record with readable fields rather than unreadable positional arguments
  also there shouldn't need to be different versions! all scripts should behave similarly
  xx functions to rewrite with scripts:
    xx MMSeqs: aMmseqsSearchDb, aMmseqsCreateDbAll, aMmseqConvertAlis
    xx BlastDB: aMakeblastdbAll
    xx Blast: aMkBlastFromDb
    xx BlastCRB: aCRBBlast
    xx Diamond: aDiamondFromDb
    xx SonicParanoid: aSonicParanoid
    xx Muscle: aMuscle
    xx Hmmer: aHmmbuild, aHmmSearch, aExtractHmm
  need to send all stdout from individual commands to intermediate files rather than read it directly
    should never be able to confuse filenames with output like this:
      "diamond_makedb.sh /global/home/users/jefdaj/.shortcut/exprs/diamond_makedb/c8116966c3_0.dmnd /global/home/users/jefdaj/.shortcut/cache/load/WARNI.faa"
    should work without assuming anything about what machine the command will be running on, or if it crashes + repeats
  this is basically separating the "eval" step into "args" and "shell" steps right?
    always write a .args/.sh file first
  write .lock files? or use the .args?
  always redirect stdout, stderr to their own files as it runs
  always rerun the command if it fails?
  should this use a separate mechanism from the berkeley wrapper or are they the same?
  should any real processing *ever* be done in the main haskell binary thread?

fix up blast stuff:
  generalize BlastHits.hs to HitTables.hs so it works with blastlike programs (CRB-Blast, DIAMOND, ...)
  should there be a separate RBH module too that works with BLAST, DIAMOND, MMSeqs2?

result handling:
  x should be able to reassign other variables without it assuming result should be recalculated immediately
  x should be able to use result in further expressions and have it replaced by its value then
  should allow lists of any/mixed type in result to make exploring easier
  should be able to zip up one archive of the result list on the website

improvements to parsing/repl:
  should error if replacing something with a different type and stuff depends on it
  label parsers with meaningful error messages where possible
  add custom errors for commonly confusing situations

idea for repeat_each/lambda expressions:
  make the lambda/placeholder/whatever expression corrsepond to a dir, and the argument to a file in it!
  the dir can have the complete hash, same as files do in most of the exprs/* dirs
  then inside that links to zero or more arguments (this makes sense even for single args I think)
  arrange like $TMPDIR/lambda/<hash rep expr with placeholder>/<hash argument expr>
  there only needs to be one of those ^ per input and no outputs right?
    the overall output can go where it would for normal compilation $TMPDIR/exprs/repeat_each/<hash>
    each completed input expression output can also be linked to where it would go
      the only catch is those have to surprise shake, but that's fine
      think this part through... can you actually compile them? or just do exprHash?
  think what to name it: lambda? promise? repeat_each2? but don't take long on that
  overall backwards evaluation:
    exprs/repeat_each2/<complete expr hash> needs + contains list of single outputs
    each single output is generated from an input like lambdas/<lambda expr hash>/<arg expr hash>
    the lambdas/<lambda expr hash> dir is filled how?

fix the unicode encoding issues!
  things to do:
    switch to binary read/write of golden tests as suggested in the docs
    stop tasty from trying to overwrite the .cut scripts during round-trip
    fix R tmpfile errors
  tests should pass on:
    demo server
    laptop vm
    hpc singularity environment
    with nix-build or nix-shell + stack build on each one

more .args files idea:
  another level of indirection solves the problem of mapping over a fn call in Compile.Map already
  but it's ugly and adds a confusing extra convention of each/<hash>... cache files
  feels like something similar will be needed to replace_each on fn calls
  maybe the simplest thing is to extend the .args convention to more/even all fn calls?
  it seems like it could also drastically simplify other maplike operations

leave_each_out bug:
  leave_each_out [1,2,3,4,5] -> works as expected
  leave_each_out [1,2,3] -> does the first 3 elements of the first list! wtf

add mmseqs_rbh stuff!
:write on its own should write to the current script after asking confirmation

type groups:
  x add the basic data type
  x handle them in the parser/typechecker functions
  add type groups for:
    x files with length in lines
    x fasta files
    x blast hits-like types
    things that can have orthogroups extracted from them
    things that can have homolog pairs extracted from them (new homologs module)
  rewrite all fTypeCheck and fTypeDesc functions using defaultTypeCheck and mkTypeDesc
  remove the option to use non-default typechecks + descriptions at all

what should and shouldn't go in the rep digests? think about that for a few minutes, then move on

file leaking bug:
  what's happening?
    somewhere, opened files aren't being properly closed when an exception occurs
    so randomly there will be some error, then they'll pile up and most remaining tests fail
    this is probably a big source of actual scripts failing too!
  also just learned that enclosed-exceptions is resource heavy and no longer recommended
  x rewrite everything with safe-exceptions!
    x find all instances of error, throw, throwIO, bracket, recoverAll, etc.
    x replace them with proper functions :D
    x eval failures having to do with the file not existing shouldn't have to be repeated
  prevent file descriptor leaks
    any reason not to make all reads strict?
    wrap all instances of reading or writing a file with cleanup (close handles)
    check that number of open file desciptors stays low during --test
      remember that stdin/stdout/everything else in the tests counts too
      things to handle:
        .show files
        makeblastdb seems to have particular issues
  try re-implementing the disk resource now and see if it keeps open files in check

bugs identified while updating tests:
  extracts_seqs broken, possibly because of the seqid_ hashing

rewrite all tests to use mycoplasma for speed and consistency
  start from the ones use by mmseqs
  need their full genomes though
  and in faa + fna format
  also include any other from https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-1801-0
  also include Nanoarchaeum equitans
parse errors (including type errors) shouldn't crash the interpreter!
biomartr: get working and improve to handle IDs from papers
spend some time thinking of a better name
add optional varnames to all cache + expr files? if not too hard it would help manual exploration
list specific functions that are nondeterministic, and maybe have conditions each should satisfy instead?
add simplest possible "typeclasses" to combine types, then remove fTypeDesc stuff
bugs to fix:
  bad bug!
    eval leave_each_out [1,2,3,4,5]
    then leave_each_out [1,2,3,4,5, 1e-45]
    the second result will be wrong:
    [[2, 3, 4, 5], [1, 3, 4, 5], [1, 2, 4, 5], [1, 2, 3, 5], [1, 2, 3, 4], [1, 2, 3, 4, 5]]
  when a var fails to parse the error shouldn't be "no such variable <var that failed to parse>"
  parentheses need to follow the left -> right rule!
simplify plots to look more figure like
get sonicparanoid and mmseqs to share tmpfiles
get sonicparanoid and orthofinder sharing common extraction function(s)
draft a greencut-like algorithm using diamond or mmseqs, not necessarily in haskell
write a function to extract_seqs from multiple fastas at once, useful here + elsewhere?
can all "extract" functions be renamed with "list"?
how to open the result cluster files?
swap _ for : in seqid: because orthofinder does that already (or is it messing up and considering those 2 species?)
  maybe better just remove it for now instead
should i edit orthofinder to use shortcut conventions? might be the fastest/clearest way if no options
  how important are the numeric indexes? if those could be made hashes everything would be easy
  and store blast hits as plain txt without the gzip for now
easy functions to write now that you've found the output files:
  orthogroup_seqs : ofr -> faa.list (directly return fasta files)
  orthogroup_ids  : ofr -> str..listlist (extract_ids . orthogroups seqs)
  orthogroups_containing (reimplement this, or what? and should it return ids?)
  orthogroup_gene_trees : ofr -> tree.list
  species_tree : ofr -> tree
  orthogroup_tree_containing : ofr -> str -> ? (can this be done easily? what if none?)
  single_copy_orthogroups : ofr -> str.list.list
  single_copy_orthogroup_sequences : ofr -> faa.list (this would be used to write the above)
should i make a thing to list + extract orthogroups by name? or will that just confuse because they're numeric?
sonicparanoid module
  x needs IDs sanitized first, probably
  x scaffolding
  x go straight to module, where ids will work
  x approx command: sonicparanoid -i <indir> -o <outdir> -sh <tmpdir> -m fast/default/etc. -noidx -d (if debug)
  outputs pairwise orthologs only; need to read them and make groups
orthogroup functions
  common interface for: orthofinder, sonicparanoid, mmseqs2
  how to make the types work? should the "orthogroups" fn just pull common info from any result?
  orthogroups            ::             any result -> str.list.list
  orthogroups_containing :: str.list -> any result -> str.list.list
  orthogroup_containing  :: str      -> any result -> str.list
  the main thing to add today is probably interpretation of the results:
    which genes appear in all sets and which don't?
    which appear in at least N sets or are missing from N sets?
    venn diagram of genes in common between sets
    to get those, will need to extract from each format + do common munging on list of lists
fix plots too in general and start showing more of them
diamond module
  read manual
    handles temporary files itself; output only appears at the end and is thrown away if inturrupted
    looks to be deterministic! same input -> same output according to the author
    things that can be set to speed it up/improve results:
      -e as low as possible
      -k as low as possible, or use --top instead
    later, optional taxonomy features?
    looks to be multithreaded by default
    any need to set the scoring matrix?
    can output the same regular blast results. winning!
      but there's an intermediate DAA (diamond alignment archive) format too. not sure if needed?
    can gzip the output
  x test running diamond and getting blast output
  x write scaffolding code
  x write useful functions
    x diamond_makedb
    x diamond_makedb_all
    x search variants: fasta + db, regular + sensitive + more sensitive, blastp + blastx
      x diamond_blastp
      x diamond_blastp_db
      x diamond_blastx
      x diamond_blastx_db
      x diamond_blastp_sensitive
      x diamond_blastp_db_sensitive
      x diamond_blastp_more_sensitive
      x diamond_blastp_db_more_sensitive
      x diamond_blastx_sensitive
      x diamond_blastx_db_sensitive
      x diamond_blastx_more_sensitive
      x diamond_blastx_db_more_sensitive
    x something to load existing databases? maybe not important yet
  x print data:
    x diamond dbinfo to print dbs
    x diamond view to print results
  x write tests
    test dependency
mmseqs module
  read about it
    should i be working with plass + linclust to get protein sequences from metagenomics datasets?
      cool for later, maybe
      megahit + prodigal is the standard alternative
    does a large number of things with a lot of internal commands
      looks like it can act like blast or psi-blast, but is also for clustering?
    can do easy-clust from fasta, but generally want to convert to mmseq db first?
      database is multiple files like blast
      is there a different kind of db file for each function output?
    there are "update" functions which in shortcut should copy + update
    any way to get it working on the demo server? if not, might be a reason to get a new one eventually
      should be doable from lab funds really!
      might not need it tho, only missing avx2 but SSE4.1 is ok
        just need to add a compiler flag?
    might require up to 500GB free space for large runs? i can get that if needed
    example use here: https://github.com/soedinglab/MMseqs2/issues/79
    read the readme
    skim the paper
  try some functions
    try the webserver!
    make sure you understand how the db stuff works
  write useful functions
    variants: regular and _db like blast
    variants: sensitive, default, fast, faster?
    x test dependency and abort if illegal instruction
    do these first:
      mmseqs_createdb
      mmseqs_search
    then the rest if needed:
      mmseqs_easy_search
      mmseqs_easy_cluster
      mmseqs_easy_linclust?
      mmseqs_createseqfiledb?
      mmseqs_cluster? not sure if needed
      mmseqs_result2flat
      mmseqs_mergeclusters?
      mmseqs_search_profile (with --iterations)?
  write tests
